{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree: \n",
    "Decision Trees are versatile Machine Learning algorithms that can perform both\n",
    "classification and regression tasks, and even multioutput tasks. They are very powerful algorithms,\n",
    "capable of fitting complex datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Example\n",
    "The following code trains a DecisionTreeClassifier on the iris dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)\n",
    "\n",
    "export_graphviz(\n",
    "tree_clf,\n",
    "out_file=\"images/iris_tree.dot\",\n",
    "feature_names=iris.feature_names[2:],\n",
    "class_names=iris.target_names,\n",
    "rounded=True,\n",
    "filled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "\n",
    "(graph,) = pydot.graph_from_dot_file('images/iris_tree.dot')\n",
    "graph.write_png('images/iris_tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/iris_tree.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How prediction works:\n",
    "Suppose you find an iris flower andyou want to classify it.\n",
    "1. You start at the root node (depth 0, at the top): this node asks whether the flower’s petal length is smaller than 2.45 cm.\n",
    "2. If it is, then you move down to the root’s left child node (depth 1, left). In this case, it is a leaf node **setosa** class.\n",
    "3. If it is not (petal length is greater than 2.45 cm) : You must move down to the root’s right child node (depth 1, right) **which is not a leaf node**.\n",
    "4. It asks anotherquestion: is the petal width smaller than 1.75 cm? \n",
    "5. If it is, Then go to the next level to the **left** which is a leaf node **versicolor** class.\n",
    "6. If it is not, Then go to the next level to the **right** which is a leaf node **virginica** class.\n",
    "\n",
    "NOTE: One of the many qualities of Decision Trees is that they require very little data preparation. In particular, they don’t require feature scaling or centering at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes calculations:\n",
    "Each node contain a condition which the question that spearate the data and it have different attributes to calculate such as samples, value, and gini.\n",
    "\n",
    "#### samples\n",
    "A node’s samples attribute counts how many training instances it applies to. For example. the iris flower dataset contains 150 training samples which are counted at the root. Then, 100 samples have petal length greater than 2.45 cm at level 1 **right** node. Furthermore, the 100 samples contains 54 versicolor class and 46 virginica class as counted in the right leafs.\n",
    "\n",
    "#### values \n",
    "A nodes's Value represents tells you how many training instances of each class this node applies to.for example, the bottom-right node applies to 0 Iris-Setosa, 1 IrisVersicolor, and 45 Iris-Virginica\n",
    "\n",
    "#### gini\n",
    "a node’s gini attribute measures its impurity: a node is **pure** (gini=0) if all training instances it applies to belong to the same class. For example, since the depth-1 left node applies only to Iris-Setosa training instances, it is pure and its gini score is 0. The following equation calculates score Gi of the ith node:\n",
    "\n",
    "$$ G_{i} = 1 - \\sum_{k=1}^{n} P_{i,k}^{2} $$\n",
    "\n",
    "Where P<sup>i,k</sup>is the ratio of class k instances among the training instances in the i<sup>th</sup> node.\n",
    "\n",
    "For example, the depth-2 left node has a gini score equal to 1 – (0/54)2 – (49/54)2 – (5/54)2 ≈ 0.168. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure shows Decision Tree’s decision boundaries. The thick vertical line represents the decision\n",
    "boundary of the root node (depth 0): petal length = 2.45 cm. Since the left area is pure (only Iris-Setosa),\n",
    "it cannot be split any further. However, the right area is impure, so the depth-1 right node splits it at petal\n",
    "width = 1.75 cm (represented by the dashed line). Since max_depth was set to 2, the Decision Tree stops\n",
    "right there. However, if you set max_depth to 3, then the two depth-2 nodes would each add another\n",
    "decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/im1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Estimation\n",
    "A Decision Tree can also estimate the probability that an instance belongs to a particular class k: first it\n",
    "traverses the tree to find the leaf node for this instance, and then it returns the ratio of training instances of\n",
    "class k in this node. For example, suppose you have found a flower whose petals are 5 cm long and 1.5\n",
    "cm wide. The corresponding leaf node is the depth-2 left node, so the Decision Tree should output the\n",
    "following probabilities: 0% for Iris-Setosa (0/54), 90.7% for Iris-Versicolor (49/54), and 9.3% for IrisVirginica (5/54). And of course if you ask it to predict the class, it should output Iris-Versicolor (class 1)\n",
    "since it has the highest probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.90740741, 0.09259259]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Impurity or Entropy\n",
    "\n",
    "The concept of entropy originated in thermodynamics as a measure of molecular disorder: entropy approaches zero when molecules are still and well ordered. It later spread to a wide variety of domains, including Shannon’s information theory, where it measures the average information content of a message. \n",
    "\n",
    "**entropy is zero when all messages are identical. In Machine Learning, it is frequently used as an impurity measure: a set’s entropy is zero when it contains instances of only one class**\n",
    "\n",
    "The following equation calculate the entropy at noe i.\n",
    "$$ H_{i} = - \\sum_{k=1}^{n} P_{i,k} log(P_{i,k}) $$\n",
    "\n",
    "Where P<sup>i,k</sup>is the ratio of class k instances among the training instances in the i<sup>th</sup> node.\n",
    "For example, For example, the depth-2 left node has an entropy equal to:\n",
    "$$ -\\frac{49}{54}log(\\frac{49}{54})-\\frac{5}{54}log(\\frac{5}{54}) \\approx 0.31 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So should you use Gini impurity or entropy? The truth is, most of the time it does not make a big\n",
    "difference: they lead to similar trees. Gini impurity is slightly faster to compute, so it is a good default.\n",
    "However, when they differ, Gini impurity tends to isolate the most frequent class in its own branch of the\n",
    "tree, while entropy tends to produce slightly more balanced trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
