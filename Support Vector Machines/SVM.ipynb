{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of\n",
    "performing linear or nonlinear classification, regression, and even outlier detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Margin Classification\n",
    "\n",
    "The following figure describes the idea of support vector machines in which, Two classes from iris datasets were used with two features only.\n",
    "\n",
    "![alt text](images/im1.png)\n",
    "\n",
    "The left plot shows the decision boundaries of three\n",
    "possible linear classifiers. The model whose decision boundary is represented by the dashed line is so\n",
    "bad that it does not even separate the classes properly. The other two models work perfectly on this\n",
    "training set, but their decision boundaries come so close to the instances that these models will probably\n",
    "not perform as well on new instances. In contrast, the solid line in the plot on the right represents the\n",
    "decision boundary of an SVM classifier; this line not only separates the two classes but also stays as far\n",
    "away from the closest training instances as possible. You can think of an SVM classifier as fitting the\n",
    "widest possible street (represented by the parallel dashed lines) between the classes. This is **called large\n",
    "margin classification**. Notice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully determined (or “supported”) by the instances located on the edge of the street. These instances are called the support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Sensitivity\n",
    "\n",
    "SVMs are sensitive to the feature scales, as you can the see in following figure: on the left plot, the vertical scale is much larger than the horizontal scale, so the widest possible street is close to horizontal. After feature scaling (e.g., using Scikit-Learn’s StandardScaler descibed in section 1 notes), the decision boundary looks much better.\n",
    "![alt text](images/im2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Margin Classification\n",
    "\n",
    "If we strictly impose that all instances be off the street and on the right side, this is called hard margin\n",
    "classification. There are two main issues with hard margin classification. First, it only works if the data\n",
    "is linearly separable, and second it is quite sensitive to outliers. se the below figure:\n",
    "\n",
    "![alt text](images/im1.png)\n",
    "![alt text](images/im3.png)\n",
    "\n",
    "The objective is to find a good balance between keeping the street as large as possible and limiting the margin violations (i.e., instances that end up in the middle of the street or even on the wrong side). This is called soft margin classification.\n",
    "\n",
    "In Scikit-Learn’s SVM classes, you can control this balance using the C hyperparameter: a smaller C value\n",
    "leads to a wider street but more margin violations. The following figure shows the decision boundaries and margins\n",
    "of two soft margin SVM classifiers on a nonlinearly separable dataset. On the left, using a high C value\n",
    "the classifier makes fewer margin violations but ends up with a smaller margin. On the right, using a low\n",
    "C value the margin is much larger, but many instances end up on the street. However, it seems likely that\n",
    "the second classifier will generalize better: in fact even on this training set it makes fewer prediction\n",
    "errors, since most of the margin violations are actually on the correct side of the decision boundary\n",
    "\n",
    "![alt text](images/im4.png)\n",
    "\n",
    "The following Scikit-Learn code loads the iris dataset, scales the features, and then trains a linear SVM\n",
    "model (using the Linear Support Vector Classifier (LinearSVC) class with C = 0.1) to detect Iris-Virginica flowers which is the right part in the previous figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) # Iris-Virginica\n",
    "svm_clf = Pipeline((\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n",
    "))\n",
    "svm_clf.fit(X, y)\n",
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you could use the SVC class, using SVC(kernel=\"linear\", C=1), but it is much slower,\n",
    "especially with large training sets, so it is not recommended.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear SVM Classification\n",
    "\n",
    "Although linear SVM classifiers are efficient and work surprisingly well in many cases, many datasets\n",
    "are not even close to being linearly separable. One approach to handling nonlinear datasets is to add more\n",
    "features, such as polynomial features  in some cases this can result in a linearly\n",
    "separable dataset. Consider the left plot in following Figure : it represents a simple dataset with just one feature x1. This dataset is not linearly separable, as you can see. But if you add a second feature x2 = (x1)2, the resulting 2D dataset is perfectly linearly separable.\n",
    "\n",
    "![alt text](images/im5.png)\n",
    "\n",
    "To implement this idea using Scikit-Learn, you can create a Pipeline containing a PolynomialFeatures\n",
    "transformer, followed by a StandardScaler and a LinearSVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('poly_features',\n",
       "                 PolynomialFeatures(degree=3, include_bias=True,\n",
       "                                    interaction_only=False, order='C')),\n",
       "                ('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('svm_clf',\n",
       "                 LinearSVC(C=10, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "                           penalty='l2', random_state=None, tol=0.0001,\n",
       "                           verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implementation\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "polynomial_svm_clf = Pipeline((\n",
    "(\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n",
    "))\n",
    "polynomial_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Kernel\n",
    "\n",
    "Adding polynomial features is simple to implement and can work great with all sorts of Machine Learning\n",
    "algorithms (not just SVMs), but at a low polynomial degree it cannot deal with very complex datasets,\n",
    "and with a high polynomial degree it creates a huge number of features, making the model too slow.\n",
    "\n",
    "Fortunately, when using SVMs you can apply an almost miraculous mathematical technique called the\n",
    "kernel trick. It makes it possible to get the same result as if you added many\n",
    "polynomial features, even with very high-degree polynomials, without actually having to add them. So\n",
    "there is no combinatorial explosion of the number of features since you don’t actually add any features.\n",
    "This trick is implemented by the SVC class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('svm_clf',\n",
       "                 SVC(C=5, cache_size=200, class_weight=None, coef0=1,\n",
       "                     decision_function_shape='ovr', degree=3,\n",
       "                     gamma='auto_deprecated', kernel='poly', max_iter=-1,\n",
       "                     probability=False, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "poly_kernel_svm_clf = Pipeline((\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "))\n",
    "poly_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code trains an SVM classifier using a 3rd-degree polynomial kernel. It is represented on the left of\n",
    "the following Figure. On the right is another SVM classifier using a 10th-degree polynomial kernel. Obviously, if\n",
    "your model is overfitting, you might want to reduce the polynomial degree. Conversely, if it is\n",
    "underfitting, you can try increasing it. The hyperparameter coef0 controls how much the model is\n",
    "influenced by high-degree polynomials versus low-degree polynomials.\n",
    "\n",
    "![alt text](images/im6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM under the hood\n",
    "The linear SVM classifier model predicts the class of a new instance x by simply computing the decision\n",
    "function\n",
    "$$ W^{T} * b = w_{1}x _{1} + ...... + w_{n}x _{n} $$\n",
    "$$ prediction = 0    if W^{T} * b < 0   $$\n",
    "$$ prediction = 1    if W^{T} * b \\geq 0 $$\n",
    "\n",
    "The following figure two-dimensional plane since this dataset has two features (petal width and petal length). The decision boundary is the set of points where the decision function is equal to 0: it is the intersection of two planes, which is a straight line (represented by the thick solid line). While, The dashed lines represent the points where the decision function is equal to 1 or –1: they are parallel and at equal distance to the decision boundary, forming a margin around it. Training a linear SVM classifier means finding the value of w and b that make this margin as wide as possible while avoiding margin violations (hard margin) or limiting them (soft margin).\n",
    "\n",
    "![alt text](images/im7.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
